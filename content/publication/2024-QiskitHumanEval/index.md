+++
title = "Qiskit HumanEval: An Evaluation Benchmark For Quantum Code Generative Models"
date = 2024-06-20
authors = ["Sanjay Vishwakarma", "Francis Harkins", "Siddharth Golecha", "Vishal Sharathchandra Bajpe", "Nicolas Dupuis", "Luca Buratti", "David Kremer", "Ismael Faro", "Ruchir Puri", "Juan Cruz-Benito"]
publication_types = ["3"]
abstract = "Quantum programs are typically developed using quantum Software Development Kits (SDKs). The rapid advancement of quantum computing necessitates new tools to streamline this development process, and one such tool could be Generative Artificial intelligence (GenAI). In this study, we introduce and use the Qiskit HumanEval dataset, a hand-curated collection of tasks designed to benchmark the ability of Large Language Models (LLMs) to produce quantum code using Qiskit - a quantum SDK. This dataset consists of more than 100 quantum computing tasks, each accompanied by a prompt, a canonical solution, a comprehensive test case, and a difficulty scale to evaluate the correctness of the generated solutions. We systematically assess the performance of a set of LLMs against the Qiskit HumanEval dataset's tasks and focus on the models ability in producing executable quantum code. Our findings not only demonstrate the feasibility of using LLMs for generating quantum code but also establish a new benchmark for ongoing advancements in the field and encourage further exploration and development of GenAI-driven tools for quantum code generation."
selected = false
publication = "*arXiv preprint arXiv:2406.14712*"
tags = ["Qiskit", "Large Language Models", "Evaluation benchmarks", "Qiskit HumanEval", "HumanEval"]
url_source = "https://arxiv.org/abs/2406.14712"
doi = "10.48550/arXiv.2406.14712"
+++
